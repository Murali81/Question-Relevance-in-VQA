{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.inception_v3 import InceptionV3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "parent_folder = \"D:/SEM 7/Core Project/MyCode/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(parent_folder +\"VQG/Image-Captioning/data_preprocessing/\" + \"img2questions_dict.pickle\",\"rb\") as fh:\n",
    "    img2questions_dict = pickle.load(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img_folder = \"VQG/Image-Captioning/VQg_Dataset/VQG_Dataset/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_files = glob.glob(parent_folder+img_folder+\"*.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img_file_names = [os.path.basename(x) for x in img_files] ## removing the path part, keeping only the filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img_id_file_map = dict()\n",
    "\n",
    "for img_fil in img_file_names:\n",
    "    img_id_file_map[str((img_fil.split(\".\"))[0] )] = img_fil \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_input(x):\n",
    "    x /= 255.\n",
    "    x -= 0.5\n",
    "    x *= 2.\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(image_path):\n",
    "    img = image.load_img(image_path, target_size=(299, 299))\n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "\n",
    "    x = preprocess_input(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = InceptionV3(weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "\n",
    "new_input = model.input\n",
    "hidden_layer = model.layers[-2].output\n",
    "\n",
    "model_new = Model(new_input, hidden_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encode(image):\n",
    "    image = preprocess(image)\n",
    "    temp_enc = model_new.predict(image)\n",
    "    temp_enc = np.reshape(temp_enc, temp_enc.shape[1])\n",
    "    return temp_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "problematic_images=''\n",
    "\n",
    "encoding_img = {}\n",
    "\n",
    "for imgid,img in tqdm(img_id_file_map.items()):\n",
    "    \n",
    "    try:\n",
    "        encoded = encode(parent_folder+img_folder+img)\n",
    "        \n",
    "        encoding_img[imgid] = encoded\n",
    "        \n",
    "    except:\n",
    "        print(img)\n",
    "        problematic_images += img+\"\\n\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "with open(parent_folder+\"VQG/Image-Captioning/VQg_Dataset/\"+\"img_vectors.pickle\",\"wb\") as fh:\n",
    "    pickle.dump(encoding_img,fh)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(parent_folder+\"VQG/Image-Captioning/VQg_Dataset/\"+\"img_vectors.pickle\",\"rb\") as fh:\n",
    "    encoding_img=pickle.load(fh)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample=encoding_img['208252'].reshape(1,2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_encoding_img = dict((k,i) for i,k in enumerate(encoding_img.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "rev_list_encoding_img = dict((i,k) for i,k in enumerate(encoding_img.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_of_imgs=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for idx in range(len(list(encoding_img.keys()))):\n",
    "    \n",
    "    img_ide = rev_list_encoding_img[idx]\n",
    "    \n",
    "    vector = encoding_img[img_ide]\n",
    "    \n",
    "    list_of_imgs.append(vector)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=300)\n",
    "compressed_encoding_img_values = pca.fit_transform(np.vstack(list_of_imgs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "compressed_encoding_img=dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(len(list(encoding_img.keys()))):\n",
    "    \n",
    "    img_ide = rev_list_encoding_img[idx]\n",
    "    \n",
    "    vector = compressed_encoding_img_values[idx]\n",
    "    \n",
    "    compressed_encoding_img[img_ide] = vector\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"D:/MyCodes/EMBEDDINGS/glove_840B_300d.pickle\",\"rb\") as fh:\n",
    "    embed_dict=pickle.load(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_dim = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_vectors=[]\n",
    "\n",
    "for idx in range(len(list(encoding_img.keys()))):\n",
    "    \n",
    "    img_ide = rev_list_encoding_img[idx]\n",
    "    \n",
    "#     img_vector = encoding_img[img_ide]\n",
    "    \n",
    "    quess = img2questions_dict[img_ide]\n",
    "    \n",
    "    img_vector = compressed_encoding_img[img_ide]\n",
    "    \n",
    "    for ques in quess:\n",
    "        \n",
    "        ques = ques[:-1]\n",
    "        ques = ques.lower()\n",
    "        \n",
    "        ques = ques.split()\n",
    "        \n",
    "        sent_v=np.zeros((embedding_dim),dtype='double')\n",
    "        for word in ques:\n",
    "            try:\n",
    "                v = embed_dict[word]\n",
    "            except:\n",
    "                v=np.zeros((embedding_dim),dtype='double')\n",
    "\n",
    "            sent_v+=v\n",
    "            \n",
    "        sent_v = sent_v / (len(ques))\n",
    "                \n",
    "        \n",
    "        final_vectors.append([img_vector,sent_v])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"img_ques_pair_vectors.pickle\",\"wb\") as fh:\n",
    "    pickle.dump(final_vectors,fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"img_ques_pair_vectors.pickle\",\"rb\") as fh:\n",
    "    final_vectors=pickle.load(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_matrix = []\n",
    "ques_matrix = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for img_vec,ques_vec in final_vectors:\n",
    "    imgs_matrix.append(img_vec)\n",
    "    ques_matrix.append(ques_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_matrix = np.vstack(imgs_matrix)\n",
    "ques_matrix = np.vstack(ques_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"imgs_matrix.pickle\",\"wb\") as fh:\n",
    "    pickle.dump(imgs_matrix,fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"ques_matrix.pickle\",\"wb\") as fh:\n",
    "    pickle.dump(ques_matrix,fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"ques_matrix.pickle\",\"rb\") as fh:\n",
    "    ques_matrix = pickle.load(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python35",
   "language": "python",
   "name": "python35"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
