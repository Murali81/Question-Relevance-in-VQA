{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "torch.cuda.init()\n",
    "device = torch.device(\"cuda:0\")\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_sequence(seq,dictionary):\n",
    "    return torch.tensor([dictionary.get(i,0) for i in seq], dtype = torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def loadGloveModel(gloveFile):\n",
    "    print (\"Loading Glove Model\")\n",
    "    f = open(gloveFile,'r', encoding='utf8')\n",
    "    model = {}\n",
    "    for line in tqdm_notebook(f):\n",
    "        splitLine = line.split(' ')\n",
    "        word = splitLine[0]\n",
    "        embedding = np.asarray(splitLine[1:], dtype='float32')\n",
    "        model[word] = embedding\n",
    "    print (\"Done.\",len(model),\" words loaded!\")\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# glove_model=loadGloveModel(\"D:/MyCodes/EMBEDDINGS/glove.840B.300d.txt\")\n",
    "\n",
    "# import pickle\n",
    "# with open(\"D:/MyCodes/EMBEDDINGS/glove_840B_300d.pickle\",\"wb\") as fh:\n",
    "#     pickle.dump(glove_model,fh)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"D:/MyCodes/EMBEDDINGS/glove_840B_300d.pickle\",\"rb\") as fh:\n",
    "    embed_dict=pickle.load(fh)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_labels(data):\n",
    "    \n",
    "    \n",
    "    targets=[]\n",
    "    \n",
    "    for ques in tqdm_notebook(data):\n",
    "        if ques[2].lower() not in targets:\n",
    "            targets.append(ques[2].lower())\n",
    "            \n",
    "    return targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"D:/SEM 7/Core Project/Dataset/Datasets/Classification_task/classification_clusters.pickle\",\"rb\") as fh:\n",
    "    total_data=pickle.load(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7940714101d4bd7a556eee009ed6fb2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "uniq_targets=create_labels(total_data)\n",
    "rev_targets=dict((v,k) for k,v in enumerate(uniq_targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def return_data(dictionary2,data):\n",
    "    \n",
    "    if dictionary2==None:\n",
    "        print(\"Dictionary is not passed\")\n",
    "        return \n",
    "    \n",
    "    encoded_totl_data=[]\n",
    "    questions=[]\n",
    "    encoded_targets=[]\n",
    "    \n",
    "    for ques in tqdm_notebook(data):\n",
    "        \n",
    "        if \"?\" in ques[0]:\n",
    "            ques[0]=ques[0].replace(\"?\",\" ?\")\n",
    "\n",
    "        \n",
    "        encoded_ques=[word.lower() for word in ques[0].split(\" \")]  # Tokenizing and encoding\n",
    "        encoded_targ=dictionary2[ques[2].lower()] # Encoding targets\n",
    "        \n",
    "        encoded_totl_data.append([encoded_ques,encoded_targ])\n",
    "        questions.append(encoded_ques)\n",
    "        encoded_targets.append(encoded_targ)\n",
    "        \n",
    "        \n",
    "    return encoded_totl_data,questions,encoded_targets\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f1e38e3c939487580cd28e11b2a9786"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "encoded_total_data,questions,encoded_targets= return_data(rev_targets,total_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab=list(embed_dict.keys())\n",
    "vocab.insert(0,\"__unk__\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_dim = 300\n",
    "hidden_dim = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ques2vec=[]\n",
    "\n",
    "for qid,question in tqdm_notebook(enumerate(questions[:2]),desc=\"Progress\"):\n",
    "    sent_v=np.zeros((embedding_dim),dtype='double')\n",
    "    for word in question:\n",
    "        try:\n",
    "            v = embed_dict[word]\n",
    "        except:\n",
    "            v=np.zeros((embedding_dim),dtype='double')\n",
    "            \n",
    "        sent_v+=v\n",
    "        \n",
    "    ques2vec.append(sent_v)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"question_id_list.pickle\",\"wb\") as fh:\n",
    "    pickle.dump(questions,fh)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"question_id_vec.pickle\",\"wb\") as fh:\n",
    "    pickle.dump(ques2vec,fh)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
